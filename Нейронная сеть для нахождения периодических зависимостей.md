## 1. Introduction

1.0 Изучая нейронные сети все глубже сталкиваешься с тем, что не ко всем задачам принименимы полносвязные глубокие нейронные сети с класическими слоями Linear и слоями активации Relue + Than.

## 1.1 Мотивация:
### Периодические функции это естественный процесс
В целом, периодические функции являются одной из самых основных функций, важных для человеческого общества
и естествознания: суточные и годовые циклы в мире диктуются периодическими движениями Солнечной системы.
Человеческому организму присущи биологические часы, которые носят периодический характер,
количество пассажиров в метро меняется ежедневно и еженедельно, а фондовый рынок
испытывает (полупериодические) колебания. 
Мировая экономика также следует сложным и накладывающимся друг на друга циклам разных периодов, включая, но не ограничиваясь циклом Китчина и Юглара
Во многих научных сценариях мы хотим смоделировать периодическую систему, чтобы иметь возможность предсказать будущую эволюцию, основываясь на текущих и прошлых наблюдениях. В то время как глубокие нейронные сети являются отличными инструментами для интерполяции между существующими данными, их фидуциарная версия не подходит для экстраполяции за пределы диапазона обучения, особенно для периодических функций.

Если мы заранее знаем, что задача является периодической, мы можем легко решить ее, помощью пребразования Фурье. Однако во многих ситуациях мы априори не знаем, является ли проблема является периодической или содержит периодический компонент. В таких случаях важно иметь универсальную модель, которая была бы достаточно гибкой для моделирования как периодических, так и непериодических функций, чтобы преодолеть предвзятость при выборе определенного подхода к моделированию. 

Существуют некоторые предыдущие методы, предлагающие использовать функции периодической активации (Fourier neural networks). В предлагается использовать периодические функции, sin(x) и cos(x), или их линейные комбинации в качестве функций активации. Однако такие функции активации очень трудно оптимизировать из-за большого вырождения локальных минимумов и экспериментальные результаты показывают, что использование sin в качестве функции активации работает плохо, за исключением очень простых моделей, и что оно не может конкурировать с функциями активации на основе ReLU о стандартных задача.

## 2. Применение стандартных подходов





